{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonglab/.local/lib/python3.8/site-packages/nilearn/datasets/__init__.py:93: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "import nibabel\n",
    "import numpy as np\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn import image\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjIDs = [\"2013\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\",\"2021\",\"2022\",\"2024\", \n",
    "\"2027\",\"2028\",\"2029\",\"2030\",\"2031\",\"2032\",\"2033\",\"2034\",\"2035\",\"2036\",\"2038\",\n",
    "\"2039\",\"2040\",\"2041\",\"2042\",\"2043\",\"2044\",\"2045\",\"2046\",\"2047\"]\n",
    "\n",
    "mask_file = \"vvs\";\n",
    "design = \"cue_align_native_minus1\"\n",
    "classifier_name = \"log_l1\" # Or 'SVC', 'log_l1', log_l1_50 or log_l2\n",
    "\n",
    "dir_input = \"../../3_results/2_glm/\" + design\n",
    "dir_label = \"z_trial_labels\"                   # needs to be in trial space\n",
    " \n",
    "dir_traininput =\"../../3_results/2_glm/align_native_delete1\"\n",
    "dir_trainlabel =\"z_trial_labels_TR\"               # needs to be in TR space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Subj: 2013\n",
      "Training using log_l1\n",
      "Running Subj: 2015\n",
      "Training using log_l1\n",
      "Running Subj: 2016\n",
      "Training using log_l1\n",
      "Running Subj: 2017\n",
      "Training using log_l1\n",
      "Running Subj: 2018\n",
      "Training using log_l1\n",
      "Running Subj: 2019\n",
      "Training using log_l1\n",
      "Running Subj: 2020\n",
      "Training using log_l1\n",
      "Running Subj: 2021\n",
      "Training using log_l1\n",
      "Running Subj: 2022\n",
      "Training using log_l1\n",
      "Running Subj: 2024\n",
      "Training using log_l1\n",
      "Running Subj: 2027\n",
      "Training using log_l1\n",
      "Running Subj: 2028\n",
      "Training using log_l1\n",
      "Running Subj: 2029\n",
      "Training using log_l1\n",
      "Running Subj: 2030\n",
      "Training using log_l1\n",
      "Running Subj: 2031\n",
      "Training using log_l1\n",
      "Running Subj: 2032\n",
      "Training using log_l1\n",
      "Running Subj: 2033\n",
      "Training using log_l1\n",
      "Running Subj: 2034\n",
      "Training using log_l1\n",
      "Running Subj: 2035\n",
      "Training using log_l1\n",
      "Running Subj: 2036\n",
      "Training using log_l1\n",
      "Running Subj: 2038\n"
     ]
    }
   ],
   "source": [
    "for subjID in subjIDs:\n",
    "    print(\"Running Subj: %s\" % subjID)\n",
    "    \n",
    "    #############################################################################\n",
    "    #                     Always train on the same lag                         #\n",
    "    #############################################################################\n",
    "        \n",
    "    # fMRI path\n",
    "    run5_path = dir_traininput + \"/\" + \"sub-\" + subjID + \"/run5.feat/filtered_func_data.nii.gz\"\n",
    "    run6_path = dir_traininput + \"/\" + \"sub-\" + subjID + \"/run6.feat/filtered_func_data.nii.gz\"\n",
    "    \n",
    "    # mask path\n",
    "    mask_path = \"../../2_fmri/masks/align_native/\" + mask_file + \"/\" + \"sub-\" + subjID + \"_run1.nii.gz\"\n",
    "    \n",
    "    # Labels\n",
    "    label_run5_path = dir_trainlabel + \"/\" + subjID + \"/Add0/run5_label.txt\"\n",
    "    label_run6_path = dir_trainlabel + \"/\" + subjID + \"/Add0/run6_label.txt\"\n",
    "    \n",
    "    # Load labels and make target\n",
    "    run5_labels = np.recfromcsv(label_run5_path, delimiter=\" \")\n",
    "    run5_target = run5_labels['labels']\n",
    "    run5_condition_mask = np.logical_or(run5_labels['labels'] == 1, run5_labels['labels'] == 2)\n",
    "    run5_target = run5_target[run5_condition_mask]\n",
    "    \n",
    "    run6_labels = np.recfromcsv(label_run6_path, delimiter=\" \")\n",
    "    run6_target = run6_labels['labels']\n",
    "    run6_condition_mask = np.logical_or(run6_labels['labels'] == 1, run6_labels['labels'] == 2)\n",
    "    run6_target = run6_target[run6_condition_mask]\n",
    "\n",
    "    # Load MRI data\n",
    "    nifti_masker = NiftiMasker(mask_img=mask_path,standardize=True)\n",
    "    run5_fmri_masked = nifti_masker.fit_transform(run5_path)    \n",
    "    run6_fmri_masked = nifti_masker.fit_transform(run6_path)\n",
    "    \n",
    "    # Need to make sure labels is same size as fMRI data\n",
    "    toPad = run5_fmri_masked.shape[0] - run5_condition_mask.size\n",
    "    padding = np.zeros(toPad, dtype = bool)\n",
    "    run5_condition_mask = np.append(run5_condition_mask,padding)\n",
    "\n",
    "    toPad = run6_fmri_masked.shape[0] - run6_condition_mask.size\n",
    "    padding = np.zeros(toPad, dtype = bool)\n",
    "    run6_condition_mask = np.append(run6_condition_mask,padding)\n",
    "    \n",
    "    # subset data\n",
    "    run5_fmri_masked = run5_fmri_masked[run5_condition_mask]\n",
    "    run6_fmri_masked = run6_fmri_masked[run6_condition_mask]\n",
    "        \n",
    "    # Concatenate Runs\n",
    "    combined_runs = np.vstack((run5_fmri_masked,run6_fmri_masked))\n",
    "        \n",
    "    # Concatenate Labels\n",
    "    combined_labels = np.hstack((run5_target,run6_target))\n",
    "    \n",
    "    # Decoder\n",
    "    if classifier_name == \"log_l1\":\n",
    "        print(\"Training using log_l1\")\n",
    "        classifier = LogisticRegression(C=1., penalty=\"l1\", solver='liblinear',tol=0.00001)  \n",
    "        \n",
    "    # Train\n",
    "    classifier.fit(combined_runs, combined_labels)\n",
    "        \n",
    "    # Create toPrint \n",
    "    AllShifts = np.zeros((0,5));\n",
    "                    \n",
    "    # Initialize variables\n",
    "    task_path = []\n",
    "    task_label_path = []\n",
    "    task_label = []\n",
    "    task_target = []\n",
    "    task_condition_mask = []\n",
    "    task_target = []\n",
    "    task_trial = []\n",
    "    task_fmri_masked = []\n",
    "    combined_task = []\n",
    "    combined_task_trial = []\n",
    "\n",
    "    # Load run 1 to run 4 \n",
    "    for r in range(0,4):                \n",
    "        task_path.append(dir_input + \"/\" + \"sub-\" + subjID + \"/run\" + str(r+1) + \"_copemap_scaled.nii.gz\")        \n",
    "        task_label_path.append(dir_label + \"/\" + subjID  + \"/run\" + str(r+1) + \"_label.txt\")\n",
    "        task_label.append(np.recfromcsv(task_label_path[r], delimiter=\" \"))\n",
    "        task_target.append(task_label[r]['condition'])\n",
    "        task_condition_mask.append(np.logical_or(task_label[r]['condition'] == 1, task_label[r]['condition'] == 2))\n",
    "        task_target[r] = task_target[r][task_condition_mask[r]]\n",
    "        task_trial.append(task_label[r][task_condition_mask[r]])\n",
    "\n",
    "        # Stopped here because of mask issues\n",
    "        task_fmri_masked.append(nifti_masker.fit_transform(task_path[r]))\n",
    "        task_fmri_masked[r] = task_fmri_masked[r][task_condition_mask[r]]\n",
    "\n",
    "    #Concatenate Runs and Labels \n",
    "    combined_task = np.vstack([task_fmri_masked[0],task_fmri_masked[1],task_fmri_masked[2],task_fmri_masked[3]])\n",
    "    combined_task_trial = np.vstack([task_trial[0].tolist(),task_trial[1].tolist(),task_trial[2].tolist(),task_trial[3].tolist()])\n",
    "\n",
    "    # Classify\n",
    "    prob = classifier.predict_proba(combined_task)\n",
    "\n",
    "    # Output to CSV\n",
    "    toPrint = np.column_stack([combined_task_trial,prob[:,1]])\n",
    "    AllShifts = np.vstack([AllShifts,toPrint]);\n",
    "\n",
    "    if not os.path.exists(\"../../3_results/6_TrialClassification/\" + design + \"_copemap_scaled/TrainTrial/\" + mask_file + \"/\" + classifier_name):\n",
    "        os.makedirs(\"../../3_results/6_TrialClassification/\" + design + \"_copemap_scaled/TrainTrial/\" + mask_file + \"/\" + classifier_name)\n",
    "\n",
    "    outputfile = \"../../3_results/6_TrialClassification/\" + design + \"_copemap_scaled/TrainTrial/\" + mask_file + \"/\" + classifier_name + \"/\" + subjID + \".csv\"\n",
    "    np.savetxt(outputfile, AllShifts, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
